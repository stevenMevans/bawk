{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a9d61bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n",
      "0.9.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division,unicode_literals\n",
    "import os\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "\n",
    "import torch.nn.functional as Fi\n",
    "\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "\n",
    "\n",
    "import string\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2119bd40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f8b4f5ee62447083067f65fe3c4a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading manifest data: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table_trans = str.maketrans(dict.fromkeys(string.punctuation))  # OR {key: None for key in string.punctuation}\n",
    "\n",
    "\n",
    "path =\"/Users/dami.osoba/work/bawk/small_dataset/small/CV_unpacked/cv-corpus-6.1-2020-12-11/en/validated.tsv\"\n",
    "meta = pd.read_csv(path,sep=\"\\t\")\n",
    "meta_path = meta.set_index('path')\n",
    "\n",
    "def read_manifest(path):\n",
    "    manifest = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in tqdm(f, desc=\"Reading manifest data\"):\n",
    "            line = line.replace(\"\\n\", \"\")\n",
    "            data = json.loads(line)\n",
    "            manifest.append(data)\n",
    "    return manifest\n",
    "train_manifest_path = '/Users/dami.osoba/work/bawk/src/data/commonvoice_train_manifest.json'\n",
    "train_manifest_data = read_manifest(train_manifest_path)\n",
    "# keep audio < 4s\n",
    "train_text = [data['text'] for data in train_manifest_data if data['duration']<=4]\n",
    "train_path = [data['audio_filepath'] for data in train_manifest_data if data['duration']<=4]\n",
    "train_path_pd = pd.DataFrame(train_path,columns=['train_path'])\n",
    "\n",
    "# remove unicode\n",
    "sentences = [c.encode(encoding=\"ascii\",errors=\"ignore\").decode().translate(table_trans) for c in train_text]\n",
    "char_dict = sorted(list(set([b for a in sentences for b in a]))) +['EOS','SOS','PAD']\n",
    "char_index = {a:char_dict.index(a) for a in char_dict}\n",
    "dictOfindex = {char_dict.index(a):a for a in char_dict}\n",
    "# char_index['EOS'] = len(char_dict)\n",
    "# char_index['SOS'] = len(char_dict)+1\n",
    "# char_index['PAD'] = len(char_dict)+2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33f9426",
   "metadata": {},
   "source": [
    "# Create voice dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4a735473",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "#         self.landmarks_frame = pd.read_csv(csv_file)\n",
    "#         self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.path_frame = train_path_pd\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.path_frame)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "#         waveform, _ = librosa.load(self.path_frame.loc[idx][0],sr=16000)\n",
    "        waveform, _ = torchaudio.load(self.path_frame.loc[idx][0],)\n",
    "        label = self.path_frame.loc[idx][0].split(\"/\")[-1].split(\"wav\")[0]+\"mp3\"\n",
    "        # transcription for audio\n",
    "        trans = meta_path.loc[label]['sentence']\n",
    "#         if len(trans.split(\" \")) == 1:\n",
    "#             waveform = torch.cat([waveform,waveform],dim=1)\n",
    "#             trans = trans +\" \"+trans\n",
    "#             print(trans)\n",
    "        # encode to ascii\n",
    "        trans = trans.encode(encoding=\"ascii\",errors=\"ignore\").decode().translate(table_trans).lower()\n",
    "        chars =[b for a in trans for b in a]\n",
    "        coded = [28]+[char_dict.index(a) for a in chars]+[27]\n",
    "\n",
    "        sample = {'waveform': waveform, 'transcription': coded,'sentence':trans}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1098d3b1",
   "metadata": {},
   "source": [
    "# Create FFT transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3b88ce88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1, 226, 80]) that sobered him a little\n",
      "1 torch.Size([1, 363, 80]) open confession is good for the soul\n",
      "2 torch.Size([1, 344, 80]) but the englishman was exultant\n",
      "3 torch.Size([1, 274, 80]) i am following my destiny\n"
     ]
    }
   ],
   "source": [
    "window_size = 25/1000\n",
    "stride = 10/1000\n",
    "sample_rate = 16000\n",
    "n_fft =int(window_size *sample_rate)\n",
    "win_length = None\n",
    "hop_length = int(sample_rate*stride)\n",
    "n_mels = 80\n",
    "max_time = 4\n",
    "\n",
    "\n",
    "mel_spectrogram = T.MelSpectrogram(\n",
    "    sample_rate=sample_rate,\n",
    "    n_fft=n_fft,\n",
    "    hop_length=hop_length,\n",
    "    center=True,\n",
    "    pad_mode=\"reflect\",\n",
    "    power=2.0,\n",
    "    norm='slaney',\n",
    "    onesided=True,\n",
    "    n_mels=n_mels,\n",
    "    mel_scale=\"htk\",\n",
    ")\n",
    "\n",
    "# melspec = mel_spectrogram(waveform)\n",
    "\n",
    "\n",
    "class MelSpec(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.window_size = 25/1000\n",
    "        self.stride = 10/1000\n",
    "        self.sample_rate = 16000\n",
    "        self.n_fft =int(self.window_size *self.sample_rate)\n",
    "        self.win_length = None\n",
    "        self.hop_length = int(self.sample_rate*self.stride)\n",
    "        self.n_mels = 80\n",
    "        self.max_time = 4\n",
    "        pass\n",
    "#         assert isinstance(output_size, (int, tuple))\n",
    "#         self.output_size = output_size\n",
    "\n",
    "    def mel_spectrogram(self,a):\n",
    "        mel_spec = T.MelSpectrogram(\n",
    "                        sample_rate=self.sample_rate,\n",
    "                        n_fft=self.n_fft,\n",
    "                        hop_length=self.hop_length,\n",
    "                        center=True,\n",
    "                        pad_mode=\"reflect\",\n",
    "                        power=2.0,\n",
    "                        norm='slaney',\n",
    "                        onesided=True,\n",
    "                        n_mels=self.n_mels,\n",
    "                        mel_scale=\"htk\")\n",
    "        return mel_spec(a)\n",
    "        \n",
    "\n",
    "    def __call__(self, sample):\n",
    "        waveform, transcription,sentence = sample['waveform'], sample['transcription'],sample['sentence']\n",
    "        #zero pad waveform\n",
    "        zero_pad = torch.zeros(1, sample_rate*max_time- waveform.size()[1])\n",
    "        padding = torch.cat([waveform,zero_pad],1)\n",
    "        # get spectrogram\n",
    "        wave_spec = self.mel_spectrogram(waveform)\n",
    "        wave_spec = wave_spec.swapaxes(1,2)\n",
    "        #change transcription list to tensor\n",
    "        transcription = torch.tensor(transcription, dtype=torch.long, device=device)\n",
    "\n",
    "        return {'waveform': wave_spec, 'transcription': transcription, 'sentence':sentence}\n",
    "    \n",
    "transformed_dataset = VoiceDataset(transform = MelSpec())\n",
    "\n",
    "for i in range(len(transformed_dataset)):\n",
    "    sample = transformed_dataset[i]\n",
    "\n",
    "    print(i, sample['waveform'].size(), sample['sentence'])\n",
    "\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f211b1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictOfindex = { i : char_dict[i] for i in range(0, len(char_dict) ) }\n",
    "dictOfchar = { char_dict[i]:i for i in range(0, len(char_dict) ) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2d858ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         ...,\n",
      "         [1.0571e-07, 3.7094e-07, 4.8095e-07,  ..., 2.9530e-07,\n",
      "          1.9673e-07, 7.4557e-08],\n",
      "         [5.0575e-08, 1.7748e-07, 4.0925e-07,  ..., 2.9312e-08,\n",
      "          9.3720e-09, 1.3724e-08],\n",
      "         [1.4446e-09, 5.0692e-09, 4.5073e-09,  ..., 6.0489e-10,\n",
      "          1.2518e-09, 5.0188e-10]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [5.6637e-09, 1.9875e-08, 1.1874e-08,  ..., 1.1692e-11,\n",
      "          2.9177e-11, 9.1143e-11],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [2.5911e-15, 9.0928e-15, 6.1791e-15,  ..., 1.2433e-14,\n",
      "          1.1812e-14, 1.0906e-14],\n",
      "         [1.3317e-09, 4.6733e-09, 9.3070e-10,  ..., 2.3414e-10,\n",
      "          4.6644e-10, 3.5086e-10],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [2.3470e-16, 8.2359e-16, 5.3478e-16,  ..., 5.1091e-16,\n",
      "          5.6350e-16, 5.9304e-16],\n",
      "         [6.4171e-08, 2.2519e-07, 1.5147e-07,  ..., 1.5529e-08,\n",
      "          2.5423e-08, 1.4854e-08],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [9.3469e-15, 3.2800e-14, 2.1327e-14,  ..., 2.4221e-14,\n",
      "          2.4332e-14, 2.4162e-14],\n",
      "         [1.5522e-11, 5.4468e-11, 1.7434e-10,  ..., 8.9165e-11,\n",
      "          7.5128e-11, 6.7543e-11],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]]]) tensor([[28, 20,  8,  5,  0,  9, 14, 20,  5, 18,  6,  1,  3,  5,  0, 23,  1, 19,\n",
      "          0,  9, 14,  4,  5,  3,  9, 16,  8,  5, 18,  1,  2, 12,  5, 27, 29, 29,\n",
      "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29],\n",
      "        [28, 20,  8,  5,  0, 12, 15, 18, 18, 25,  0,  4, 18,  9, 22,  5, 18,  0,\n",
      "         25,  1, 23, 14,  5,  4,  0,  1, 14,  4,  0,  4,  5,  3,  9,  4,  5,  4,\n",
      "          0, 20, 15,  0,  8,  5,  1,  4,  0,  6, 15, 18,  0, 20,  8,  5,  0, 14,\n",
      "          5, 24, 20,  0, 13, 15, 20,  5, 12, 27],\n",
      "        [28, 19,  8,  5,  0, 12,  5,  1, 18, 14,  5,  4,  0,  8, 15, 23,  0, 20,\n",
      "         15,  0, 16, 12,  1, 25,  0, 20,  8,  5,  0, 16,  9,  1, 14, 15,  0,  9,\n",
      "         14,  0,  1,  0, 13, 21, 19,  9,  3,  0, 19,  3,  8, 15, 15, 12, 27, 29,\n",
      "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29],\n",
      "        [28,  2,  1,  4,  8,  1, 13,  0,  1, 12, 19, 15,  0, 16, 21,  2, 12,  9,\n",
      "         19,  8,  5,  4,  0, 19, 15, 13,  5,  0,  3, 18,  9, 20,  9, 17, 21,  5,\n",
      "         19,  0, 15,  6,  0, 19,  8,  1, 11,  5, 19, 16,  5,  1, 18,  5, 27, 29,\n",
      "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29],\n",
      "        [28,  2, 21, 18, 14,  0, 16,  5,  1, 20,  0,  1,  6, 20,  5, 18,  0, 20,\n",
      "          8,  5,  0, 12, 15,  7, 19,  0,  7,  9, 22,  5,  0, 15, 21, 20, 27, 29,\n",
      "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29],\n",
      "        [28, 25, 15, 21,  0,  1, 12,  1, 18, 13,  0, 13,  5,  0, 19,  1,  9,  4,\n",
      "          0, 20,  8,  5,  0, 11,  9, 14,  7, 27, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29],\n",
      "        [28, 14,  9, 14,  5, 27, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29],\n",
      "        [28, 23,  8,  1, 20,  0,  4, 15,  5, 19,  0, 20,  8,  9, 19,  0, 13,  5,\n",
      "          1, 14, 27, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29],\n",
      "        [28, 16, 21, 20,  0,  4, 15, 23, 14,  0, 20,  8,  1, 20,  0,  3,  8,  1,\n",
      "          9, 18, 27, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29],\n",
      "        [28, 19,  5, 22,  5, 14, 27, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29]]) tensor([397, 387, 382, 368, 303, 301, 284, 255, 236, 159]) ('the interface was indecipherable', 'the lorry driver yawned and decided to head for the next motel', 'she learned how to play the piano in a music school', 'badham also published some critiques of shakespeare', 'burn peat after the logs give out', 'you alarm me said the king', 'nine', 'what does this mean', 'put down that chair', 'seven')\n"
     ]
    }
   ],
   "source": [
    "def pad_sequence(batch):\n",
    "    # Make all tensor in a batch the same length by padding with zeros\n",
    "    batch = [item.t() for item in batch]    \n",
    "    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0)\n",
    "    return batch\n",
    "\n",
    "\n",
    "def pad_collate(batch):\n",
    "    max_input_len = float('-inf')\n",
    "    max_target_len = float('-inf')\n",
    "\n",
    "    for elem in batch:\n",
    "        feature = elem['waveform']\n",
    "        feature = feature.squeeze()\n",
    "        trn = elem['transcription']\n",
    "        max_input_len = max_input_len if max_input_len > feature.shape[0] else feature.shape[0]\n",
    "        max_target_len = max_target_len if max_target_len > len(trn) else len(trn)\n",
    "\n",
    "    for i, elem in enumerate(batch):\n",
    "        f = elem['waveform']\n",
    "        trn = elem['transcription']\n",
    "        sentence = elem['sentence']\n",
    "        f = f.squeeze()\n",
    "        input_length = f.shape[0]\n",
    "        input_dim = f.shape[1]\n",
    "        # print('f.shape: ' + str(f.shape))\n",
    "        feature = np.zeros((max_input_len, input_dim), dtype=np.float32)\n",
    "        feature[:f.shape[0], :f.shape[1]] = f\n",
    "        trn = np.pad(trn, (0, max_target_len - len(trn)), 'constant', constant_values=29)\n",
    "        batch[i] = (feature, trn, input_length,sentence)\n",
    "        # print('feature.shape: ' + str(feature.shape))\n",
    "        # print('trn.shape: ' + str(trn.shape))\n",
    "\n",
    "    batch.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    return default_collate(batch)\n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    # A data tuple has the form:\n",
    "    # waveform, sample_rate, label, speaker_id, utterance_number\n",
    "\n",
    "    tensors, targets,sentence = [], [],[]\n",
    "\n",
    "    # Gather in lists, and encode labels as indices\n",
    "    for a in batch:\n",
    "        tensors += [a['waveform']]\n",
    "        targets += [a['transcription']]\n",
    "        sentence += [a['sentence']]\n",
    "                   \n",
    "    # Group the list of tensors into a batched tensor\n",
    "    tensors = tensors\n",
    "#     targets = torch.stack(targets)\n",
    "    targets = pad_sequence(targets)\n",
    "\n",
    "    return tensors, targets,sentence\n",
    "\n",
    "\n",
    "train_loader = DataLoader(transformed_dataset, batch_size=10,collate_fn=pad_collate,\n",
    "                        shuffle=True, num_workers=0)\n",
    "\n",
    "iterator = iter(train_loader)\n",
    "x_batch,y,input_lengths,sentence = iterator.next()\n",
    "print(x_batch,y,input_lengths,sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a345081b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[28, 20,  8,  5,  0,  9, 14, 20,  5, 18,  6,  1,  3,  5,  0, 23,  1, 19,\n",
       "          0,  9, 14,  4,  5,  3,  9, 16,  8,  5, 18,  1,  2, 12,  5, 27, 29, 29,\n",
       "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
       "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29],\n",
       "        [28, 20,  8,  5,  0, 12, 15, 18, 18, 25,  0,  4, 18,  9, 22,  5, 18,  0,\n",
       "         25,  1, 23, 14,  5,  4,  0,  1, 14,  4,  0,  4,  5,  3,  9,  4,  5,  4,\n",
       "          0, 20, 15,  0,  8,  5,  1,  4,  0,  6, 15, 18,  0, 20,  8,  5,  0, 14,\n",
       "          5, 24, 20,  0, 13, 15, 20,  5, 12, 27],\n",
       "        [28, 19,  8,  5,  0, 12,  5,  1, 18, 14,  5,  4,  0,  8, 15, 23,  0, 20,\n",
       "         15,  0, 16, 12,  1, 25,  0, 20,  8,  5,  0, 16,  9,  1, 14, 15,  0,  9,\n",
       "         14,  0,  1,  0, 13, 21, 19,  9,  3,  0, 19,  3,  8, 15, 15, 12, 27, 29,\n",
       "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29],\n",
       "        [28,  2,  1,  4,  8,  1, 13,  0,  1, 12, 19, 15,  0, 16, 21,  2, 12,  9,\n",
       "         19,  8,  5,  4,  0, 19, 15, 13,  5,  0,  3, 18,  9, 20,  9, 17, 21,  5,\n",
       "         19,  0, 15,  6,  0, 19,  8,  1, 11,  5, 19, 16,  5,  1, 18,  5, 27, 29,\n",
       "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29],\n",
       "        [28,  2, 21, 18, 14,  0, 16,  5,  1, 20,  0,  1,  6, 20,  5, 18,  0, 20,\n",
       "          8,  5,  0, 12, 15,  7, 19,  0,  7,  9, 22,  5,  0, 15, 21, 20, 27, 29,\n",
       "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
       "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29],\n",
       "        [28, 25, 15, 21,  0,  1, 12,  1, 18, 13,  0, 13,  5,  0, 19,  1,  9,  4,\n",
       "          0, 20,  8,  5,  0, 11,  9, 14,  7, 27, 29, 29, 29, 29, 29, 29, 29, 29,\n",
       "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
       "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29],\n",
       "        [28, 14,  9, 14,  5, 27, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
       "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
       "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
       "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29],\n",
       "        [28, 23,  8,  1, 20,  0,  4, 15,  5, 19,  0, 20,  8,  9, 19,  0, 13,  5,\n",
       "          1, 14, 27, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
       "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
       "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29],\n",
       "        [28, 16, 21, 20,  0,  4, 15, 23, 14,  0, 20,  8,  1, 20,  0,  3,  8,  1,\n",
       "          9, 18, 27, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
       "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
       "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29],\n",
       "        [28, 19,  5, 22,  5, 14, 27, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
       "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
       "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
       "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fcfb74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = transformed_dataset[10]['waveform']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26c4da2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 363, 80])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e01f5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = nn.GRU(80,10)\n",
    "n2 = nn.GRU(10, 10)\n",
    "n3 = nn.GRU(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "26027fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "o1,o2 = n1(x_batch)\n",
    "b1,b2 = n2(o1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "17076086",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm = nn.GRU(80,10,bidirectional=True)\n",
    "hmm2 = nn.GRU(20,5,bidirectional=True)\n",
    "input_x = x_batch.size(1)\n",
    "enc_len = x_batch.size(2)\n",
    "total_length = x_batch.size(1)\n",
    "packed_input = pack_padded_sequence(x_batch, input_lengths, batch_first=True)\n",
    "hah, oo= hmm(packed_input,oo)\n",
    "wq,tt = hmm2(hah)\n",
    "output, _ = pad_packed_sequence(wq, batch_first=True, total_length=total_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "72360b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 399, 80])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "834ec297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 399, 10])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dc0ab85b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9, 20,  0,  3,  1, 14,  0,  8, 15, 23,  5, 22,  5, 18,  0,  2,  5,  0,\n",
       "         19, 12,  1,  9, 14,  0, 23,  9, 20,  8,  0,  1,  0, 19,  9, 12, 22,  5,\n",
       "         18,  0,  2, 21, 12, 12,  5, 20],\n",
       "        [ 9, 12, 12,  0, 15, 23, 14,  0, 20,  8,  1, 20,  0,  8,  5, 19,  0, 23,\n",
       "          9, 20,  8,  0,  1,  0,  8,  1, 18, 19,  8,  0, 19,  5, 20, 29, 29, 29,\n",
       "         29, 29, 29, 29, 29, 29, 29, 29],\n",
       "        [ 4, 15,  0, 18,  9,  7,  8, 20,  0,  1, 14,  4,  0,  6,  5,  1, 18,  0,\n",
       "         14, 15,  0, 13,  1, 14, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
       "         29, 29, 29, 29, 29, 29, 29, 29],\n",
       "        [ 1,  0, 13,  1, 14,  0,  9, 19,  0, 11, 14, 15, 23, 14,  0,  2, 25,  0,\n",
       "         20,  8,  5,  0,  3, 15, 13, 16,  1, 14, 25,  0,  8,  5,  0, 11,  5,  5,\n",
       "         16, 19, 29, 29, 29, 29, 29, 29],\n",
       "        [20,  8,  1, 20,  0, 19, 15,  2,  5, 18,  5,  4,  0,  8,  9, 13,  0,  1,\n",
       "          0, 12,  9, 20, 20, 12,  5, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
       "         29, 29, 29, 29, 29, 29, 29, 29],\n",
       "        [19,  8,  5,  0, 23, 15, 21, 12,  4,  0, 14,  5, 22,  5, 18,  0, 19, 16,\n",
       "          5,  1, 11,  0, 20, 15,  0,  8,  9, 13, 29, 29, 29, 29, 29, 29, 29, 29,\n",
       "         29, 29, 29, 29, 29, 29, 29, 29],\n",
       "        [ 4,  9, 19,  3, 15, 22,  5, 18, 25,  0,  2,  1, 25,  0,  8,  1, 19,  0,\n",
       "          1,  0, 19,  8, 15, 18, 20,  0,  8,  9, 19, 20, 15, 18, 25, 29, 29, 29,\n",
       "         29, 29, 29, 29, 29, 29, 29, 29],\n",
       "        [ 1,  0, 19, 20, 21,  6,  6,  5,  4,  0,  3,  8,  1,  9, 18,  0, 19, 12,\n",
       "          9, 16, 16,  5,  4,  0,  6, 18, 15, 13,  0, 20,  8,  5,  0, 13, 15, 22,\n",
       "          9, 14,  7,  0, 22,  1, 14, 29],\n",
       "        [20,  8,  9, 19,  0, 22,  9,  5, 23,  0,  9, 19,  0, 23, 18, 15, 14,  7,\n",
       "          0, 20,  8, 15, 21,  7,  8, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
       "         29, 29, 29, 29, 29, 29, 29, 29],\n",
       "        [25, 15, 21,  0, 11, 14, 15, 23,  0,  8,  5, 18,  0,  2,  5, 20, 20,  5,\n",
       "         18,  0, 19, 15,  0, 25, 15, 21,  0, 19,  8, 15, 21, 12,  4,  0,  4, 15,\n",
       "          0,  9, 20, 29, 29, 29, 29, 29]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2608219c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_list(xs, pad_value):\n",
    "        # From: espnet/src/nets/e2e_asr_th.py: pad_list()\n",
    "        n_batch = len(xs)\n",
    "        max_len = max(x.size(0) for x in xs)\n",
    "        pad = xs[0].new(n_batch, max_len, *xs[0].size()[1:]).fill_(pad_value)\n",
    "        for i in range(n_batch):\n",
    "            pad[i, :xs[i].size(0)] = xs[i]\n",
    "        return pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e35c2f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = [yi[yi != 29] for yi in y]  # parse padded ys\n",
    "# prepare input and output word sequences with sos/eos IDs\n",
    "eos = ys[0].new([27])\n",
    "sos = ys[0].new([28])\n",
    "ys_in = [y for y in ys]\n",
    "ys_out = [y for y in ys]\n",
    "# padding for ys with -1\n",
    "# pys: utt x olen\n",
    "ys_in_pad = pad_list(ys_in, 27)\n",
    "ys_out_pad = pad_list(ys_out, 29)\n",
    "assert ys_in_pad.size() == ys_out_pad.size()\n",
    "batch_size = ys_in_pad.size(0)\n",
    "output_length = ys_in_pad.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ff862bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9, 20,  0,  3,  1, 14,  0,  8, 15, 23,  5, 22,  5, 18,  0,  2,  5,  0,\n",
       "         19, 12,  1,  9, 14,  0, 23,  9, 20,  8,  0,  1,  0, 19,  9, 12, 22,  5,\n",
       "         18,  0,  2, 21, 12, 12,  5, 20],\n",
       "        [ 9, 12, 12,  0, 15, 23, 14,  0, 20,  8,  1, 20,  0,  8,  5, 19,  0, 23,\n",
       "          9, 20,  8,  0,  1,  0,  8,  1, 18, 19,  8,  0, 19,  5, 20, 29, 29, 29,\n",
       "         29, 29, 29, 29, 29, 29, 29, 29],\n",
       "        [ 4, 15,  0, 18,  9,  7,  8, 20,  0,  1, 14,  4,  0,  6,  5,  1, 18,  0,\n",
       "         14, 15,  0, 13,  1, 14, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
       "         29, 29, 29, 29, 29, 29, 29, 29],\n",
       "        [ 1,  0, 13,  1, 14,  0,  9, 19,  0, 11, 14, 15, 23, 14,  0,  2, 25,  0,\n",
       "         20,  8,  5,  0,  3, 15, 13, 16,  1, 14, 25,  0,  8,  5,  0, 11,  5,  5,\n",
       "         16, 19, 29, 29, 29, 29, 29, 29],\n",
       "        [20,  8,  1, 20,  0, 19, 15,  2,  5, 18,  5,  4,  0,  8,  9, 13,  0,  1,\n",
       "          0, 12,  9, 20, 20, 12,  5, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
       "         29, 29, 29, 29, 29, 29, 29, 29],\n",
       "        [19,  8,  5,  0, 23, 15, 21, 12,  4,  0, 14,  5, 22,  5, 18,  0, 19, 16,\n",
       "          5,  1, 11,  0, 20, 15,  0,  8,  9, 13, 29, 29, 29, 29, 29, 29, 29, 29,\n",
       "         29, 29, 29, 29, 29, 29, 29, 29],\n",
       "        [ 4,  9, 19,  3, 15, 22,  5, 18, 25,  0,  2,  1, 25,  0,  8,  1, 19,  0,\n",
       "          1,  0, 19,  8, 15, 18, 20,  0,  8,  9, 19, 20, 15, 18, 25, 29, 29, 29,\n",
       "         29, 29, 29, 29, 29, 29, 29, 29],\n",
       "        [ 1,  0, 19, 20, 21,  6,  6,  5,  4,  0,  3,  8,  1,  9, 18,  0, 19, 12,\n",
       "          9, 16, 16,  5,  4,  0,  6, 18, 15, 13,  0, 20,  8,  5,  0, 13, 15, 22,\n",
       "          9, 14,  7,  0, 22,  1, 14, 29],\n",
       "        [20,  8,  9, 19,  0, 22,  9,  5, 23,  0,  9, 19,  0, 23, 18, 15, 14,  7,\n",
       "          0, 20,  8, 15, 21,  7,  8, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
       "         29, 29, 29, 29, 29, 29, 29, 29],\n",
       "        [25, 15, 21,  0, 11, 14, 15, 23,  0,  8,  5, 18,  0,  2,  5, 20, 20,  5,\n",
       "         18,  0, 19, 15,  0, 25, 15, 21,  0, 19,  8, 15, 21, 12,  4,  0,  4, 15,\n",
       "          0,  9, 20, 29, 29, 29, 29, 29]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c149fdab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9, 20,  0,  3,  1, 14,  0,  8, 15, 23,  5, 22,  5, 18,  0,  2,  5,  0,\n",
       "         19, 12,  1,  9, 14,  0, 23,  9, 20,  8,  0,  1,  0, 19,  9, 12, 22,  5,\n",
       "         18,  0,  2, 21, 12, 12,  5, 20],\n",
       "        [ 9, 12, 12,  0, 15, 23, 14,  0, 20,  8,  1, 20,  0,  8,  5, 19,  0, 23,\n",
       "          9, 20,  8,  0,  1,  0,  8,  1, 18, 19,  8,  0, 19,  5, 20, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27],\n",
       "        [ 4, 15,  0, 18,  9,  7,  8, 20,  0,  1, 14,  4,  0,  6,  5,  1, 18,  0,\n",
       "         14, 15,  0, 13,  1, 14, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27],\n",
       "        [ 1,  0, 13,  1, 14,  0,  9, 19,  0, 11, 14, 15, 23, 14,  0,  2, 25,  0,\n",
       "         20,  8,  5,  0,  3, 15, 13, 16,  1, 14, 25,  0,  8,  5,  0, 11,  5,  5,\n",
       "         16, 19, 27, 27, 27, 27, 27, 27],\n",
       "        [20,  8,  1, 20,  0, 19, 15,  2,  5, 18,  5,  4,  0,  8,  9, 13,  0,  1,\n",
       "          0, 12,  9, 20, 20, 12,  5, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27],\n",
       "        [19,  8,  5,  0, 23, 15, 21, 12,  4,  0, 14,  5, 22,  5, 18,  0, 19, 16,\n",
       "          5,  1, 11,  0, 20, 15,  0,  8,  9, 13, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27],\n",
       "        [ 4,  9, 19,  3, 15, 22,  5, 18, 25,  0,  2,  1, 25,  0,  8,  1, 19,  0,\n",
       "          1,  0, 19,  8, 15, 18, 20,  0,  8,  9, 19, 20, 15, 18, 25, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27],\n",
       "        [ 1,  0, 19, 20, 21,  6,  6,  5,  4,  0,  3,  8,  1,  9, 18,  0, 19, 12,\n",
       "          9, 16, 16,  5,  4,  0,  6, 18, 15, 13,  0, 20,  8,  5,  0, 13, 15, 22,\n",
       "          9, 14,  7,  0, 22,  1, 14, 27],\n",
       "        [20,  8,  9, 19,  0, 22,  9,  5, 23,  0,  9, 19,  0, 23, 18, 15, 14,  7,\n",
       "          0, 20,  8, 15, 21,  7,  8, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27],\n",
       "        [25, 15, 21,  0, 11, 14, 15, 23,  0,  8,  5, 18,  0,  2,  5, 20, 20,  5,\n",
       "         18,  0, 19, 15,  0, 25, 15, 21,  0, 19,  8, 15, 21, 12,  4,  0,  4, 15,\n",
       "          0,  9, 20, 27, 27, 27, 27, 27]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys_in_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffc06547",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout=0.0, bidirectional=True):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout,\n",
    "                          bidirectional=bidirectional)\n",
    "        self.rnn2 = nn.GRU(hidden_size, hidden_size)\n",
    "        \n",
    "\n",
    "    def forward(self, input_x, enc_len):\n",
    "        total_length = input_x.size(1)  # get the max sequence length\n",
    "        # print('total_length: ' + str(total_length))\n",
    "        # print('input_x.size(): ' + str(input_x.size()))\n",
    "        packed_input = pack_padded_sequence(input_x, enc_len, batch_first=True)\n",
    "        # print('enc_len: ' + str(enc_len))\n",
    "        packed_output, hidden = self.rnn(packed_input)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True, total_length=total_length)\n",
    "        return output, hidden\n",
    "    \n",
    "hmm = nn.GRU(80,10,bidirectional=True)\n",
    "input_x = x_batch.size(1)\n",
    "enc_len = x_batch.size(2)\n",
    "total_length =x_batch.size(1)\n",
    "packed_input = pack_padded_sequence(x_batch, input_lengths, batch_first=True)\n",
    "hah, _= hmm(packed_input)\n",
    "output, _ = pad_packed_sequence(hah, batch_first=True, total_length=total_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da621abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 399, 20])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6d7bc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(80, 30, 1, dropout=0.0, bidirectional=False)\n",
    "\n",
    "gg, oo = encoder(x_batch,input_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12bc3927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 399, 30])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ed15eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 30])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "ff1a4cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(80, 30, 1, dropout=0.0, bidirectional=True)\n",
    "decoder = Decoder(vocab_size=29, embedding_dim=15, hidden_size=30, num_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1c160e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 63])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "1b4deac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ww, wo ,wi = decoder(y,gg,oo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "fefe8968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([640, 29]), torch.Size([640]), torch.Size([10, 64]))"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ww.shape, wo.shape,wi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "d33f5d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm = Fi.cross_entropy(ww, wo,\n",
    "                                  ignore_index=29,\n",
    "                                  reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "07c6f897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9204, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "5f324e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3766, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "f4a3ce5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([364])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm[hmm >0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "93967c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363.9755967541314"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1229/3.3766"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "82091318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnny = nn.ModuleList()\n",
    "rnny = nn.LSTMCell(15, 30,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a19fa2d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-188-9282dd77480b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m         return F.embedding(\n\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2041\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2043\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "em = decoder.embedding(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0eb0a9",
   "metadata": {},
   "source": [
    "ln = em.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "87f858df",
   "metadata": {},
   "outputs": [],
   "source": [
    "wah = rnny(em[:,0,:],(oo[0],oo[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "721c138d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-163-d0a8c5de1353>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwah\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "wah[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d6fd4bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 15])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em[:,i,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "58d6afd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(em.size(1)):\n",
    "    h,c = rnny(em[:,i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "304bf7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        # Hyper parameters\n",
    "        # embedding + output\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        # rnn\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.encoder_hidden_size = hidden_size  # must be equal now\n",
    "        # Components\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.gru = nn.ModuleList()\n",
    "        self.gru += [nn.LSTMCell(self.embedding_dim, self.hidden_size)]\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.encoder_hidden_size,\n",
    "                      self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.vocab_size))\n",
    "\n",
    "    def zero_state(self, encoder_padded_outputs, H=None):\n",
    "        N = encoder_padded_outputs.size(0)\n",
    "        H = self.hidden_size if H == None else H\n",
    "        return encoder_padded_outputs.new_zeros(N, H)\n",
    "    \n",
    "    def pad_list(self,xs, pad_value):\n",
    "        # From: espnet/src/nets/e2e_asr_th.py: pad_list()\n",
    "        n_batch = len(xs)\n",
    "        max_len = max(x.size(0) for x in xs)\n",
    "        pad = xs[0].new(n_batch, max_len, *xs[0].size()[1:]).fill_(pad_value)\n",
    "        for i in range(n_batch):\n",
    "            pad[i, :xs[i].size(0)] = xs[i]\n",
    "        return pad\n",
    "    \n",
    "    def get_pads(self,padded_input):\n",
    "        PAD_token = dictOfchar['PAD']\n",
    "        EOS_token = dictOfchar['EOS']\n",
    "        SOS_token = dictOfchar['SOS']\n",
    "        ys = [y[y != PAD_token] for y in padded_input]  # parse padded ys\n",
    "        # prepare input and output word sequences with sos/eos IDs\n",
    "#         eos = ys[0].new([EOS_token])\n",
    "#         sos = ys[0].new([SOS_token])\n",
    "#         ys_in = [torch.cat([sos, y], dim=0) for y in ys]\n",
    "#         ys_out = [torch.cat([y, eos], dim=0) for y in ys]\n",
    "        # padding for ys with -1\n",
    "        # pys: utt x olen\n",
    "        ys_in_pad = self.pad_list(ys, EOS_token)\n",
    "        ys_out_pad = self.pad_list(ys, PAD_token)\n",
    "        assert ys_in_pad.size() == ys_out_pad.size()\n",
    "        \n",
    "        return ys_in_pad, ys_out_pad\n",
    "\n",
    "    def forward(self, padded_input, encoder_padded_outputs,encoder_hidden):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            padded_input: N x To\n",
    "            # encoder_hidden: (num_layers * num_directions) x N x H\n",
    "            encoder_padded_outputs: N x Ti x H\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        # *********Get Input and Output\n",
    "        # from espnet/Decoder.forward()\n",
    "        \n",
    "#         ys_in_pad, ys_out_pad = self.get_pads(padded_input)\n",
    "        \n",
    "        ys_in_pad = padded_input\n",
    "        ys_out_pad = padded_input\n",
    "\n",
    "        batch_size = ys_in_pad.size(0)\n",
    "        output_length = ys_in_pad.size(1)\n",
    "             \n",
    "        # *********Init decoder rnn\n",
    "        h_list = encoder_hidden[0]\n",
    "        c_list = encoder_hidden[1]\n",
    "\n",
    "        y_all = []\n",
    "        \n",
    "\n",
    "        # **********LAS: 1. decoder rnn 2. attention 3. concate and MLP\n",
    "        embedded = self.embedding(ys_in_pad)\n",
    "        for t in range(output_length):\n",
    "            # step 1. decoder RNN: s_i = RNN(s_i1,y_i1,c_i1)\n",
    "\n",
    "            rnn_input = embedded[:, t, :]\n",
    "            h_list, c_list = self.gru[0](\n",
    "                rnn_input, (h_list, c_list))\n",
    "            rnn_output = h_list # below unsqueeze: (N x H) -> (N x 1 x H)\n",
    "            mlp_input = rnn_output\n",
    "            predicted_y_t = self.mlp(mlp_input)\n",
    "            y_all.append(predicted_y_t)\n",
    "\n",
    "        y_all = torch.stack(y_all, dim=1)  # N x To x C\n",
    "\n",
    "        # **********Cross Entropy Loss\n",
    "        # F.cross_entropy = NLL(log_softmax(input), target))\n",
    "        y_all = y_all.view(batch_size * output_length, self.vocab_size)\n",
    "#         ce_loss = F.cross_entropy(y_all, ys_out_pad.view(-1),\n",
    "#                                   ignore_index=PAD_token,\n",
    "#                                   reduction='mean')\n",
    "\n",
    "        return y_all, ys_out_pad.view(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "2cda03f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EOS_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c4b53798",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = dictOfchar['PAD']\n",
    "EOS_token = dictOfchar['EOS']\n",
    "SOS_token = dictOfchar['SOS']\n",
    "\n",
    "\n",
    "def train(features, encoder, decoder, encoder_optimizer,\n",
    "          decoder_optimizer, criterion):\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_tensor = features[0]\n",
    "    target_tensor = features[1]\n",
    "    input_length = features[2]\n",
    "    \n",
    "    batch_size = input_tensor.size(0)\n",
    "    \n",
    "    encoder_output, encoder_hidden = encoder(input_tensor,input_length)\n",
    "    pred,actual = decoder(target_tensor,encoder_output,encoder_hidden)\n",
    "    loss = criterion(pred,actual,ignore_index=PAD_token,reduction='mean')\n",
    "    \n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "79eb0847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e6fc43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6af292ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "\n",
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "    \n",
    "    lns = len(transformed_dataset)\n",
    "\n",
    "    encoder_optimizer = optim.ADAM(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.ADAM(decoder.parameters(), lr=learning_rate)\n",
    "                      \n",
    "#     criterion = nn.NLLLoss()\n",
    "    criterion = Fi.cross_entropy\n",
    "    \n",
    "\n",
    "\n",
    "    for i in range(1, n_iters):\n",
    "        rand_sampler = torch.utils.data.RandomSampler(transformed_dataset, num_samples=10, replacement=True)\n",
    "        train_sampler = DataLoader(transformed_dataset, batch_size=11, sampler=rand_sampler,collate_fn=pad_collate)\n",
    "        iterator = iter(train_sampler)        \n",
    "        \n",
    "        features = iterator.next()            \n",
    "        loss = train(features, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if i % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, i/ n_iters),\n",
    "                                         i, i / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if i % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / (plot_every*1.0)\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f291172d",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dbb40829",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_sampler = torch.utils.data.RandomSampler(transformed_dataset, num_samples=10, replacement=True)\n",
    "train_sampler = DataLoader(transformed_dataset, batch_size=10, sampler=rand_sampler,collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375b2b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataLoader(transformed_dataset, batch_size=10,collate_fn=pad_collate,\n",
    "                        shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "b5b8c8db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function iter>"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bb895e04",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-23d6e305f3a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'decoder' is not defined"
     ]
    }
   ],
   "source": [
    "trainIters(encoder,decoder, 1000, print_every=1000, plot_every=2000,learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "cc1f56aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 37s (- 5m 40s) (100 10%) 3.3931\n",
      "1m 14s (- 4m 58s) (200 20%) 3.3420\n",
      "1m 50s (- 4m 17s) (300 30%) 3.2889\n",
      "2m 25s (- 3m 38s) (400 40%) 3.2373\n",
      "3m 1s (- 3m 1s) (500 50%) 3.1831\n",
      "3m 37s (- 2m 25s) (600 60%) 3.1304\n",
      "4m 13s (- 1m 48s) (700 70%) 3.0799\n",
      "4m 49s (- 1m 12s) (800 80%) 3.0354\n",
      "5m 25s (- 0m 36s) (900 90%) 2.9940\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(80, 30, 1, dropout=0.05, bidirectional=True)\n",
    "decoder = Decoder(vocab_size=30, embedding_dim=15, hidden_size=30, num_layers=1)\n",
    "\n",
    "trainIters(encoder,decoder, 1000, print_every=100, plot_every=2000,learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ee14aeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, features, max_length=100):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = samp[0]\n",
    "        target_tensor = samp[1]\n",
    "        input_length = samp[2]\n",
    "        decoded_words = []\n",
    "\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor,input_length)\n",
    "        dec_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "        h_list = encoder_hidden[0] \n",
    "        c_list = encoder_hidden[1]\n",
    "\n",
    "        for c in range(max_length):\n",
    "            decoder_input = decoder.embedding(dec_input)\n",
    "#             print(decoder_input.shape)\n",
    "            rng = decoder_input.size(1)\n",
    "            for rn in range(rng):\n",
    "                h_list, c_list = decoder.gru[0](decoder_input[:,rn,:], (h_list, c_list))\n",
    "            mlp_input = h_list\n",
    "            predicted_y_t = decoder.mlp(mlp_input)\n",
    "            local_scores = Fi.log_softmax(predicted_y_t, dim=1)\n",
    "            # topk scores\n",
    "            topv, topi  = torch.topk(local_scores,1,dim=1)\n",
    "            output_probs = torch.exp(local_scores)\n",
    "#             yay = torch.distributions.categorical.Categorical(output_probs)\n",
    "#             topi = yay.sample().reshape(1,1)\n",
    "#             torch.cat([dec_input,topi],dim=1)\n",
    "\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('EOS')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(dictOfindex[topi.item()])\n",
    "            dec_input = topi\n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "c4e27c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/Users/dami.osoba/work/bawk/models/dec_model_new\"\n",
    "torch.save(decoder, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "35396d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/Users/dami.osoba/work/bawk/models/enc_model_new\"\n",
    "torch.save(encoder, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6d840b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttnDecoderRNN(\n",
       "  (embedding): Embedding(29, 20)\n",
       "  (attn): Linear(in_features=40, out_features=401, bias=True)\n",
       "  (attn_combine): Linear(in_features=40, out_features=20, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (gru): GRU(20, 20)\n",
       "  (out): Linear(in_features=20, out_features=29, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enmodel = torch.load(\"/Users/dami.osoba/work/bawk/models/enc_model\")\n",
    "enmodel.eval()\n",
    "\n",
    "decmodel = torch.load(\"/Users/dami.osoba/work/bawk/models/dec_model\")\n",
    "decmodel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9624dc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_1 = DataLoader(transformed_dataset, batch_size=1,collate_fn=pad_collate,\n",
    "                        shuffle=True, num_workers=0)\n",
    "iterator = iter(samp_1) \n",
    "samp = iterator.next()\n",
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        iterator = iter(samp_1) \n",
    "        samp = iterator.next()\n",
    "        actual = samp[3]\n",
    "        output_words = evaluate(encoder, decoder, samp, max_length=100)\n",
    "        output_sentence = ' '.join(output_words[1:-1])\n",
    "        print(actual, '<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "871174a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('leko resigned when checkmate was threatened',)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e1af3bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('oh no answered the woodman',) < SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n",
      "('if a pen has no ink its broken',) < SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n",
      "('they are not listed here',) < SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n",
      "('i have not decided yet',) < SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n",
      "('try to have the court decide the case',) < SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n",
      "('factory workers work with yellow packages',) < SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n",
      "('new buildings went up as enrollments increased',) < SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n",
      "('discovery bay has a short history',) < SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n",
      "('how did it happen',) < SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n",
      "('the machine is in a sorry condition',) < SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder, decoder, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a120702d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'that might be all right.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "63b68607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that might be all right < SOSthe dige tone<EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict(train_path[23],train_text[23],enmodel,decmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ba5932a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(wav_path,transcription,encoder,decoder):\n",
    "    # Use the model to predict the label of the waveform\n",
    "    waveform, _ = torchaudio.load(wav_path)\n",
    "    sentence = transcription.encode(encoding=\"ascii\",errors=\"ignore\").decode().translate(table_trans)\n",
    "    chars =[b for a in sentence for b in a]\n",
    "    coded = [28]+[char_dict.index(a) for a in chars]+[27]\n",
    "    sample['waveform'] =waveform\n",
    "    sample['transcription'] = coded\n",
    "    sample['sentence'] = sentence\n",
    "    transformer = MelSpec()\n",
    "    mels =transformer(sample)\n",
    "    ex =mels['waveform']\n",
    "    \n",
    "    output_words, attentions,_ = evaluate(encoder, decoder, ex)\n",
    "    output_sentence = ''.join(output_words)\n",
    "    return output_sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2921f3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, tens, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tens\n",
    "        input_length = input_tensor.size(2)\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor.reshape(1,1,80*401), encoder_hidden,MAX_LENGTH)\n",
    "\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_output = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention,decoder_probs = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            yay = torch.distributions.categorical.Categorical(decoder_probs)\n",
    "            topi = yay.sample()\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(dictOfindex[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1],decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e563ee97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        choice = np.random.randint(200)\n",
    "        print(choice)\n",
    "        actual = transformed_dataset[choice]['sentence']\n",
    "        ex = transformed_dataset[choice]['waveform']\n",
    "        output_words, attentions,_ = evaluate(encoder, decoder, ex)\n",
    "        output_sentence = ''.join(output_words)\n",
    "        print(actual, '<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c23854a",
   "metadata": {},
   "source": [
    "# Really bad model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9862442e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168\n",
      "you said it to me too < SOSthet hy one ill ster a troulenpf<EOS>\n",
      "\n",
      "183\n",
      "now look what youve done < SOScoof honing ipler<EOS>\n",
      "\n",
      "13\n",
      "two < SOSyo lrlieb cowiovere zoon<EOS>\n",
      "\n",
      "62\n",
      "he disappeared into the tent < SOShegiver a of a the marze her erpeesen up eaniap browndoyrewid for adyorcrawntent witt dearntnged on u<EOS>\n",
      "\n",
      "166\n",
      "ill talk to you tonight < SOSyou poet fee hy odous courldengtas i sotellndingty on feny ontouenilhan<EOS>\n",
      "\n",
      "27\n",
      "how did it happen < SOSthee loong uperee right spigel ven<EOS>\n",
      "\n",
      "157\n",
      "richard has gone camping by himself < SOShe it fakrieve auntpy a seaits salingedirs t neees feccid<EOS>\n",
      "\n",
      "170\n",
      "what shall we do now < SOSwhot mir they inrey<EOS>\n",
      "\n",
      "152\n",
      "good weekend is edited by amelia lester < SOSjex do ge goes lhth<EOS>\n",
      "\n",
      "11\n",
      "say what you have got to say < SOSser<EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0199baf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f8daa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
