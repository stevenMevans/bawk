{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cd6090d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n",
      "0.9.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division,unicode_literals\n",
    "import os\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "import torch.nn.functional as Fi\n",
    "\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "\n",
    "\n",
    "import string\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2119bd40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "281e37efa2f34e90b092794b6bb55c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading manifest data: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table_trans = str.maketrans(dict.fromkeys(string.punctuation))  # OR {key: None for key in string.punctuation}\n",
    "\n",
    "\n",
    "path =\"/Users/dami.osoba/work/bawk/small_dataset/small/CV_unpacked/cv-corpus-6.1-2020-12-11/en/validated.tsv\"\n",
    "meta = pd.read_csv(path,sep=\"\\t\")\n",
    "meta_path = meta.set_index('path')\n",
    "\n",
    "def read_manifest(path):\n",
    "    manifest = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in tqdm(f, desc=\"Reading manifest data\"):\n",
    "            line = line.replace(\"\\n\", \"\")\n",
    "            data = json.loads(line)\n",
    "            manifest.append(data)\n",
    "    return manifest\n",
    "train_manifest_path = '/Users/dami.osoba/work/bawk/small_dataset/commonvoice_train_manifest.json'\n",
    "train_manifest_data = read_manifest(train_manifest_path)\n",
    "# keep audio < 4s\n",
    "train_text = [data['text'] for data in train_manifest_data if data['duration']<=4]\n",
    "train_path = [data['audio_filepath'] for data in train_manifest_data if data['duration']<=4]\n",
    "train_path_pd = pd.DataFrame(train_path,columns=['train_path'])\n",
    "\n",
    "# remove unicode\n",
    "sentences = [c.encode(encoding=\"ascii\",errors=\"ignore\").decode().translate(table_trans) for c in train_text]\n",
    "char_dict = sorted(list(set([b for a in sentences for b in a])))\n",
    "char_index = {a:char_dict.index(a) for a in char_dict}\n",
    "dictOfindex = {char_dict.index(a):a for a in char_dict}\n",
    "char_index['EOS'] = len(char_dict)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "1a4c5f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/dami.osoba/work/bawk/small_dataset/small/train/wav/common_voice_en_203526.wav'"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path[0].replace('src/data','small_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece307d9",
   "metadata": {},
   "source": [
    "# Create voice dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a1c1193",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "#         self.landmarks_frame = pd.read_csv(csv_file)\n",
    "#         self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.path_frame = train_path_pd\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.path_frame)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        waveform, _ = torchaudio.load(self.path_frame.loc[idx][0].replace('src/data','small_dataset'),)\n",
    "        label = self.path_frame.loc[idx][0].split(\"/\")[-1].split(\"wav\")[0]+\"mp3\"\n",
    "        # transcription for audio\n",
    "        trans = meta_path.loc[label]['sentence']\n",
    "        # encode to ascii\n",
    "        trans = trans.encode(encoding=\"ascii\",errors=\"ignore\").decode().translate(table_trans).lower()\n",
    "        chars =[b for a in trans for b in a]\n",
    "        coded = [char_dict.index(a) for a in chars]+[27]\n",
    "\n",
    "        sample = {'waveform': waveform, 'transcription': coded,'sentence':trans}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7700402",
   "metadata": {},
   "source": [
    "# Create FFT transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3d76d124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1, 401, 80]) tensor([20,  8,  1, 20,  0, 19, 15,  2,  5, 18,  5,  4,  0,  8,  9, 13,  0,  1,\n",
      "         0, 12,  9, 20, 20, 12,  5, 27]) that sobered him a little\n",
      "1 torch.Size([1, 401, 80]) tensor([15, 16,  5, 14,  0,  3, 15, 14,  6,  5, 19, 19,  9, 15, 14,  0,  9, 19,\n",
      "         0,  7, 15, 15,  4,  0,  6, 15, 18,  0, 20,  8,  5,  0, 19, 15, 21, 12,\n",
      "        27]) open confession is good for the soul\n",
      "2 torch.Size([1, 401, 80]) tensor([ 2, 21, 20,  0, 20,  8,  5,  0,  5, 14,  7, 12,  9, 19,  8, 13,  1, 14,\n",
      "         0, 23,  1, 19,  0,  5, 24, 21, 12, 20,  1, 14, 20, 27]) but the englishman was exultant\n",
      "3 torch.Size([1, 401, 80]) tensor([ 9,  0,  1, 13,  0,  6, 15, 12, 12, 15, 23,  9, 14,  7,  0, 13, 25,  0,\n",
      "         4,  5, 19, 20,  9, 14, 25, 27]) i am following my destiny\n"
     ]
    }
   ],
   "source": [
    "window_size = 25/1000\n",
    "stride = 10/1000\n",
    "sample_rate = 16000\n",
    "n_fft =int(window_size *sample_rate)\n",
    "win_length = None\n",
    "hop_length = int(sample_rate*stride)\n",
    "n_mels = 80\n",
    "max_time = 4\n",
    "\n",
    "\n",
    "mel_spectrogram = T.MelSpectrogram(\n",
    "    sample_rate=sample_rate,\n",
    "    n_fft=n_fft,\n",
    "    hop_length=hop_length,\n",
    "    center=True,\n",
    "    pad_mode=\"reflect\",\n",
    "    power=2.0,\n",
    "    norm='slaney',\n",
    "    onesided=True,\n",
    "    n_mels=n_mels,\n",
    "    mel_scale=\"htk\",\n",
    ")\n",
    "\n",
    "# melspec = mel_spectrogram(waveform)\n",
    "\n",
    "\n",
    "class MelSpec(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.window_size = 25/1000\n",
    "        self.stride = 10/1000\n",
    "        self.sample_rate = 16000\n",
    "        self.n_fft =int(self.window_size *self.sample_rate)\n",
    "        self.win_length = None\n",
    "        self.hop_length = int(self.sample_rate*self.stride)\n",
    "        self.n_mels = 80\n",
    "        self.max_time = 4\n",
    "        pass\n",
    "#         assert isinstance(output_size, (int, tuple))\n",
    "#         self.output_size = output_size\n",
    "\n",
    "    def mel_spectrogram(self,a):\n",
    "        mel_spec = T.MelSpectrogram(\n",
    "                        sample_rate=self.sample_rate,\n",
    "                        n_fft=self.n_fft,\n",
    "                        hop_length=self.hop_length,\n",
    "                        center=True,\n",
    "                        pad_mode=\"reflect\",\n",
    "                        power=2.0,\n",
    "                        norm='slaney',\n",
    "                        onesided=True,\n",
    "                        n_mels=self.n_mels,\n",
    "                        mel_scale=\"htk\")\n",
    "        return mel_spec(a)\n",
    "        \n",
    "\n",
    "    def __call__(self, sample):\n",
    "        waveform, transcription,sentence = sample['waveform'], sample['transcription'],sample['sentence']\n",
    "        #zero pad waveform\n",
    "        zero_pad = torch.zeros(1, sample_rate*max_time- waveform.size()[1])\n",
    "        padding = torch.cat([waveform,zero_pad],1)\n",
    "        # get spectrogram\n",
    "        wave_spec = self.mel_spectrogram(padding)\n",
    "        wave_spec = wave_spec.swapaxes(1,2)\n",
    "        #change transcription list to tensor\n",
    "        transcription = torch.tensor(transcription, dtype=torch.long, device=device)\n",
    "\n",
    "        return {'waveform': wave_spec, 'transcription': transcription, 'sentence':sentence}\n",
    "    \n",
    "transformed_dataset = VoiceDataset(transform = MelSpec())\n",
    "\n",
    "for i in range(len(transformed_dataset)):\n",
    "    sample = transformed_dataset[i]\n",
    "\n",
    "    print(i, sample['waveform'].size(), sample['transcription'],sample['sentence'])\n",
    "\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "6fc762ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictOfindex = { i : char_dict[i] for i in range(0, len(char_dict) ) }\n",
    "dictOfchar = { char_dict[i]:i for i in range(0, len(char_dict) ) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "520a2f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import default_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "cb04a509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399\n",
      "399\n",
      "399\n",
      "399\n",
      "399\n",
      "399\n",
      "399\n",
      "399\n",
      "399\n",
      "399\n",
      "tensor([[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [1.2996e-14, 4.5604e-14, 2.9653e-14,  ..., 3.3676e-14,\n",
      "          3.3830e-14, 3.3594e-14],\n",
      "         [1.4453e-10, 5.0718e-10, 8.4498e-10,  ..., 2.9822e-11,\n",
      "          2.5825e-11, 2.4776e-11],\n",
      "         ...,\n",
      "         [3.3585e-08, 1.1786e-07, 9.8517e-08,  ..., 1.2780e-09,\n",
      "          1.3241e-09, 6.2286e-10],\n",
      "         [1.5616e-08, 5.4800e-08, 1.1578e-07,  ..., 2.9410e-09,\n",
      "          1.0053e-09, 8.3122e-10],\n",
      "         [5.8468e-09, 2.0518e-08, 3.2361e-08,  ..., 1.3928e-09,\n",
      "          1.7140e-09, 9.3718e-10]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [6.9870e-09, 2.4519e-08, 1.2209e-08,  ..., 1.2242e-10,\n",
      "          2.2546e-11, 9.4249e-11],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [1.0255e-13, 3.5987e-13, 2.2030e-13,  ..., 1.9359e-13,\n",
      "          2.4070e-13, 1.5682e-13],\n",
      "         [6.9984e-11, 2.4559e-10, 1.4984e-10,  ..., 1.2835e-09,\n",
      "          2.3660e-10, 1.7565e-10],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [1.5945e-13, 5.5953e-13, 3.5691e-13,  ..., 1.4065e-13,\n",
      "          2.5788e-13, 1.4142e-13],\n",
      "         [4.8421e-09, 1.6992e-08, 1.9610e-08,  ..., 4.7447e-10,\n",
      "          2.9575e-10, 1.4803e-10],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [6.5833e-16, 2.3102e-15, 1.5000e-15,  ..., 7.2605e-17,\n",
      "          6.9044e-17, 6.5539e-17],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]]]) tensor([[ 1,  0,  2, 18, 15, 23, 14,  0,  4, 15,  7,  0, 18, 21, 14, 19,  0,  9,\n",
      "         14,  0, 20,  8,  5,  0,  7, 18,  1, 19, 19,  0, 23,  9, 20,  8,  0, 15,\n",
      "         14,  5,  0,  5,  1, 18,  0, 21, 16, 27,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0],\n",
      "        [ 6,  1, 19, 20,  5, 14,  0, 20, 23, 15,  0, 16,  9, 14, 19,  0, 15, 14,\n",
      "          0,  5,  1,  3,  8,  0, 19,  9,  4,  5, 27,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0],\n",
      "        [16,  5, 15, 16, 12,  5,  0, 19,  1, 23,  0, 13,  5,  0,  3, 15, 13,  9,\n",
      "         14,  7,  0,  1, 14,  4,  0, 23,  5, 12,  3, 15, 13,  5,  4,  0, 13,  5,\n",
      "          0,  8,  5,  0, 20,  8, 15, 21,  7,  8, 20, 27,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0],\n",
      "        [15, 16,  5, 14,  0,  3, 15, 14,  6,  5, 19, 19,  9, 15, 14,  0,  9, 19,\n",
      "          0,  7, 15, 15,  4,  0,  6, 15, 18,  0, 20,  8,  5,  0, 19, 15, 21, 12,\n",
      "         27,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0],\n",
      "        [ 9,  0, 20,  8, 15, 21,  7,  8, 20,  0, 20,  8,  1, 20,  0, 19, 15, 13,\n",
      "          5,  4,  1, 25,  0,  9,  4,  0,  2,  5,  0, 18,  9,  3,  8,  0,  1, 14,\n",
      "          4,  0,  3, 15, 21, 12,  4,  0,  7, 15,  0, 20, 15,  0, 13,  5,  3,  3,\n",
      "          1, 27],\n",
      "        [ 6,  9, 22,  5, 27,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0],\n",
      "        [13,  1, 25,  2,  5,  0,  8,  5, 12, 12,  0,  7,  9, 22,  5,  0, 21, 19,\n",
      "          0, 15, 14,  5, 27,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0],\n",
      "        [19,  8,  5,  0,  3, 15, 21, 12,  4,  0, 14, 15, 20,  0, 13,  1, 14,  1,\n",
      "          7,  5,  0, 20, 15,  0,  3, 12,  9, 13,  2,  0, 15, 22,  5, 18,  0, 20,\n",
      "          8,  5,  0,  6,  5, 14,  3,  5, 27,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0],\n",
      "        [20,  8,  5,  0, 15,  1, 19,  9, 19,  0,  9, 19,  0, 14,  5, 21, 20, 18,\n",
      "          1, 12,  0,  7, 18, 15, 21, 14,  4, 27,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0],\n",
      "        [ 8, 20, 13, 12,  0,  9, 19,  0, 14, 15, 20,  0,  1,  0, 16, 18, 15,  7,\n",
      "         18,  1, 13, 13,  9, 14,  7,  0, 12,  1, 14,  7, 21,  1,  7,  5, 27,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0]]) tensor([399, 392, 382, 363, 351, 337, 329, 303, 250, 241]) ('a brown dog runs in the grass with one ear up', 'fasten two pins on each side', 'people saw me coming and welcomed me he thought', 'open confession is good for the soul', 'i thought that someday id be rich and could go to mecca', 'five', 'maybe hell give us one', 'she could not manage to climb over the fence', 'the oasis is neutral ground', 'html is not a programming language')\n"
     ]
    }
   ],
   "source": [
    "def pad_sequence(batch):\n",
    "    # Make all tensor in a batch the same length by padding with zeros\n",
    "    batch = [item.t() for item in batch]    \n",
    "    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.)\n",
    "    return batch\n",
    "\n",
    "\n",
    "def pad_collate(batch):\n",
    "    max_input_len = float('-inf')\n",
    "    max_target_len = float('-inf')\n",
    "\n",
    "    for elem in batch:\n",
    "        feature = elem['waveform']\n",
    "        feature = feature.squeeze()\n",
    "        trn = elem['transcription']\n",
    "        max_input_len = max_input_len if max_input_len > feature.shape[0] else feature.shape[0]\n",
    "        max_target_len = max_target_len if max_target_len > len(trn) else len(trn)\n",
    "\n",
    "    for i, elem in enumerate(batch):\n",
    "        f = elem['waveform']\n",
    "        trn = elem['transcription']\n",
    "        sentence = elem['sentence']\n",
    "        f = f.squeeze()\n",
    "        input_length = f.shape[0]\n",
    "        input_dim = f.shape[1]\n",
    "        print(max_input_len)\n",
    "        # print('f.shape: ' + str(f.shape))\n",
    "        feature = np.zeros((max_input_len, input_dim), dtype=np.float32)\n",
    "        feature[:f.shape[0], :f.shape[1]] = f\n",
    "        trn = np.pad(trn, (0, max_target_len - len(trn)), 'constant', constant_values=0)\n",
    "        batch[i] = (feature, trn, input_length,sentence)\n",
    "        # print('feature.shape: ' + str(feature.shape))\n",
    "        # print('trn.shape: ' + str(trn.shape))\n",
    "\n",
    "    batch.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    return default_collate(batch)\n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    # A data tuple has the form:\n",
    "    # waveform, sample_rate, label, speaker_id, utterance_number\n",
    "\n",
    "    tensors, targets,sentence = [], [],[]\n",
    "\n",
    "    # Gather in lists, and encode labels as indices\n",
    "    for a in batch:\n",
    "        tensors += [a['waveform']]\n",
    "        targets += [a['transcription']]\n",
    "        sentence += [a['sentence']]\n",
    "                   \n",
    "    # Group the list of tensors into a batched tensor\n",
    "    tensors = tensors\n",
    "#     targets = torch.stack(targets)\n",
    "    targets = pad_sequence(targets)\n",
    "\n",
    "    return tensors, targets,sentence\n",
    "\n",
    "\n",
    "train_loader = DataLoader(transformed_dataset, batch_size=10,collate_fn=pad_collate,\n",
    "                        shuffle=True, num_workers=0)\n",
    "\n",
    "iterator = iter(train_loader)\n",
    "x_batch,y,input_lengths,sentence = iterator.next()\n",
    "print(x_batch,y,input_lengths,sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee4277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout=0.0, bidirectional=True):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout,\n",
    "                          bidirectional=bidirectional)\n",
    "\n",
    "    def forward(self, input_x, enc_len):\n",
    "        total_length = input_x.size(1)  # get the max sequence length\n",
    "        # print('total_length: ' + str(total_length))\n",
    "        # print('input_x.size(): ' + str(input_x.size()))\n",
    "        packed_input = pack_padded_sequence(input_x, enc_len, batch_first=True)\n",
    "        # print('enc_len: ' + str(enc_len))\n",
    "        packed_output, hidden = self.rnn(packed_input)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True, total_length=total_length)\n",
    "        return output, hidden\n",
    "    \n",
    "hmm = nn.GRU(80,10,)\n",
    "input_x = x_batch.size(1)\n",
    "enc_len = x_batch.size(2)\n",
    "total_length =x_batch.size(1)\n",
    "packed_input = pack_padded_sequence(x_batch, input_lengths, batch_first=True)\n",
    "hah, _= hmm(packed_input)\n",
    "output, _ = pad_packed_sequence(hah, batch_first=True, total_length=total_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "68684c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "#         self.embedding = nn.Embedding(input_size, hidden_size,)\n",
    "        self.embedding = nn.GRU(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden,total_length):\n",
    "        output,_ = self.embedding(input)\n",
    "        \n",
    "        \n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        m = nn.MaxPool1d(MAX_LENGTH)\n",
    "        yy = m(hidden.swapaxes(1,2))\n",
    "        yy = yy.swapaxes(1,2)\n",
    "        return output, yy\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, MAX_LENGTH, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "6155cee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "#         output = self.embedding(input)\n",
    "        print(output.shape)\n",
    "        output = Fi.relu(output)\n",
    "        print(hidden.shape,output.shape)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1,1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "847d087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "#         embedded ,_= self.embedding(input)\n",
    "\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = Fi.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = Fi.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = Fi.log_softmax(self.out(output[0]), dim=1)\n",
    "        output_probs = torch.exp(output)\n",
    "        return output, hidden, attn_weights,output_probs\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "8042e235",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.80\n",
    "\n",
    "SOS_token = 28\n",
    "EOS_token = 27\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer,\n",
    "          decoder_optimizer, criterion,total_length, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(1)\n",
    "    tot_length = total_length\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "#     for ei in range(input_length):\n",
    "#         encoder_output, encoder_hidden = encoder(\n",
    "#             input_tensor[:,:,ei].view(1,1,80), encoder_hidden)\n",
    "#         encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "#     encoder_output, encoder_hidden = encoder(input_tensor.reshape(1,MAX_LENGTH,80), encoder_hidden)\n",
    "\n",
    "    encoder_output, encoder_hidden = encoder(input_tensor, encoder_hidden,tot_length)\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention,output_probs = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention,output_probs = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            yay = torch.distributions.categorical.Categorical(output_probs)\n",
    "            topi = yay.sample()\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "1218f69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 1\n",
    "MAX_LENGTH = 401\n",
    "SOS_token = 28\n",
    "EOS_token = 27\n",
    "\n",
    "\n",
    "def train_dec(input_tensor, target_tensor, encoder, decoder, encoder_optimizer,\n",
    "          decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(2)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "#     for ei in range(input_length):\n",
    "#         encoder_output, encoder_hidden = encoder(\n",
    "#             input_tensor[:,:,ei].view(1,1,80), encoder_hidden)\n",
    "#         encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    encoder_output, encoder_hidden = encoder(input_tensor.reshape(1,MAX_LENGTH,80), encoder_hidden)\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_probs= decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden,decoder_probs = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            yay = torch.distributions.categorical.Categorical(decoder_probs)\n",
    "            topi = yay.sample()\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "de555b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "36e4954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "\n",
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "    \n",
    "    lns = len(transformed_dataset)\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_examples = np.random.choice(lns-1,n_iters)\n",
    "                      \n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters):\n",
    "        training_pair = transformed_dataset[training_examples[iter-1]]\n",
    "        input_tensor = training_pair['waveform']\n",
    "        target_tensor = torch.tensor(training_pair['transcription'], dtype=torch.long, device=device).view(-1, 1) \n",
    "        tot = input_tensor.size(1)\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion,tot)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / (plot_every*1.0)\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "ec4075bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dami.osoba/opt/anaconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 5s (- 2m 21s) (200 4%) 1.8154\n",
      "0m 11s (- 2m 10s) (400 8%) 1.8000\n",
      "0m 17s (- 2m 6s) (600 12%) 1.7867\n",
      "0m 22s (- 1m 59s) (800 16%) 1.8090\n",
      "0m 28s (- 1m 53s) (1000 20%) 1.8454\n",
      "0m 34s (- 1m 47s) (1200 24%) 1.8118\n",
      "0m 40s (- 1m 43s) (1400 28%) 1.8259\n",
      "0m 46s (- 1m 38s) (1600 32%) 1.7823\n",
      "0m 51s (- 1m 32s) (1800 36%) 1.8145\n",
      "0m 57s (- 1m 25s) (2000 40%) 1.8017\n",
      "1m 2s (- 1m 19s) (2200 44%) 1.8077\n",
      "1m 7s (- 1m 13s) (2400 48%) 1.7915\n",
      "1m 13s (- 1m 7s) (2600 52%) 1.8023\n",
      "1m 18s (- 1m 1s) (2800 56%) 1.7559\n",
      "1m 23s (- 0m 55s) (3000 60%) 1.7795\n",
      "1m 27s (- 0m 49s) (3200 64%) 1.7794\n",
      "1m 32s (- 0m 43s) (3400 68%) 1.7794\n",
      "1m 36s (- 0m 37s) (3600 72%) 1.7110\n",
      "1m 41s (- 0m 32s) (3800 76%) 1.7982\n",
      "1m 46s (- 0m 26s) (4000 80%) 1.8029\n",
      "1m 52s (- 0m 21s) (4200 84%) 1.7905\n",
      "1m 57s (- 0m 16s) (4400 88%) 1.7212\n",
      "2m 2s (- 0m 10s) (4600 92%) 1.7771\n",
      "2m 6s (- 0m 5s) (4800 96%) 1.7364\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, attn_decoder1, 5000, print_every=200, plot_every=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6707269",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "8a452508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dami.osoba/opt/anaconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 2s (- 0m 10s) (100 20%) 2.8779\n",
      "0m 4s (- 0m 7s) (200 40%) 2.5967\n",
      "0m 7s (- 0m 4s) (300 60%) 2.4585\n",
      "0m 9s (- 0m 2s) (400 80%) 2.4133\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 30\n",
    "encoder1 = EncoderRNN(80, hidden_size).to(device)\n",
    "attn_decoder2 = DecoderRNN(hidden_size, 29).to(device)\n",
    "\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, 29, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 500, print_every=100, plot_every=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "8ad14f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        choice = np.random.randint(200)\n",
    "        print(choice)\n",
    "        actual = transformed_dataset[choice]['sentence']\n",
    "        ex = transformed_dataset[choice]['waveform']\n",
    "        output_words, attentions,_ = evaluate(encoder, decoder, ex)\n",
    "        output_sentence = ''.join(output_words)\n",
    "        print(actual, '<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4343185",
   "metadata": {},
   "source": [
    "# Really bad model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "b38ebafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "the current director is mohammad sarafraz < a king ant tosible on thi proning it wucks<EOS>\n",
      "\n",
      "76\n",
      "the children are not in school in june < is and print is is looks inturs<EOS>\n",
      "\n",
      "168\n",
      "you said it to me too < the link imads<EOS>\n",
      "\n",
      "56\n",
      "see you in the funny papers < what<EOS>\n",
      "\n",
      "49\n",
      "what time will you be heading across < i thinkt is to<EOS>\n",
      "\n",
      "129\n",
      "what do you expect from a pig but a grunt < i wold in<EOS>\n",
      "\n",
      "39\n",
      "yes < in<EOS>\n",
      "\n",
      "95\n",
      "nine < brengleviry perfippamad sain her pupsunimul for broim<EOS>\n",
      "\n",
      "197\n",
      "its not so bad < wickere in a fill hickake<EOS>\n",
      "\n",
      "89\n",
      "a half volley is a difficult shot to make < burl here now beini file for prrfall of the leccay<EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
